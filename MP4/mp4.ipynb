{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3693bd7c-7b18-435a-9e8a-e9d35a75af0e",
   "metadata": {},
   "source": [
    "# MP4: Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd96d7-0f85-4f39-8499-0c692c934c62",
   "metadata": {},
   "source": [
    "In this MP, you will learn how to test and train a transformer, which is a sequence-to-sequence neural network in which the relationship between input and output is defined exclusively using dot-product attention.  You will test and train the transformer on some super-simplified natural-language tasks that would be hard to learn using a fully-connected network, though some types of recurrent networks would work as well as a transformer.\n",
    "\n",
    "If you're reading this webpage online, then the first thing you should do is download the template package:\n",
    "* [https://courses.engr.illinois.edu/ece448/sp2025/mp4/template.zip](MP4 template package)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b8ae72-5942-4114-abe6-315a54623128",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af53ebf-bfca-4d35-8f65-5efbeea0f09b",
   "metadata": {},
   "source": [
    "The template includes data, vocabulary, and a pre-trained model for a couple of different natural language tasks.  All of these files are plaintext, so you can read them with a text editor. In order to help you understand their format, though, we have provided a `reader.py` file with some `load` and `save` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2e7c5-d912-4738-aa74-22c03415d2b1",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "For example, let's try loading the vocabulary for task 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "8621f9c8-c15c-425b-82b1-ee24c538f465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_vocabulary in module reader:\n",
      "\n",
      "load_vocabulary(vocabularyfile)\n",
      "    @param:\n",
      "    vocabularyfile (str) - name of a file that contains a list of unique words\n",
      "    @return:\n",
      "    vocabulary (list) - list of those words, in the same order they had in the file\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib, reader\n",
    "help(reader.load_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "55fb1843-50b7-41cb-8e43-82361ce7bbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'swims', 'happily', 'noun', 'verb', 'adverb', 'is', 'in', 'the', 'string', 'of', 'words']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = reader.load_vocabulary('data/task1_vocabulary.txt')\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304da97-4f1d-4db9-abe8-07c54443fdd4",
   "metadata": {},
   "source": [
    "### Dev Data\n",
    "\n",
    "Task 1 doesn't have any training data, because you will be using dev data to test the provided model.  Try loading the dev data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "fb1651ea-c8b1-4e1e-b145-6f514b1bf7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_data in module reader:\n",
      "\n",
      "load_data(datafile, vocabulary)\n",
      "    Load data from a datafile.\n",
      "    \n",
      "    @param:\n",
      "    datafile (str) - the input filename, one sentence per line\n",
      "    vocabulary (list) - a list of words in the vocabulary, length V\n",
      "    \n",
      "    @return:\n",
      "    sentences - a list of N sentences, each of which is a list of T words. \n",
      "    embeddings - a list of N numpy arrays, each of size T[n]-by-V\n",
      "      embeddings[n][t,:] is one-hot embedding of the t'th word in the n'th sentence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(reader.load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "4ce6ef5d-da24-47e7-94ce-6f91fdff54dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the noun in the happily swims cat is cat\n",
      "the adverb in the string of happily cat swims is happily\n",
      "in the string happily swims cat the verb is swims\n",
      "the noun in happily cat swims is cat\n",
      "the noun in the string of swims happily cat is cat\n",
      "in the string of words swims cat happily the adverb is happily\n",
      "the verb in the string of words swims happily cat is swims\n",
      "the adverb in the string of swims cat happily is happily\n",
      "the noun in the string of words cat happily swims is cat\n",
      "in the string of swims happily cat the noun is cat\n",
      "the noun in the string happily cat swims is cat\n",
      "the adverb in the string of happily swims cat is happily\n",
      "the adverb in the string swims cat happily is happily\n",
      "in swims cat happily the verb is swims\n",
      "the verb in the string cat happily swims is swims\n",
      "the adverb in the string of cat swims happily is happily\n",
      "the verb in the string of happily cat swims is swims\n",
      "the noun in the string swims cat happily is cat\n",
      "the adverb in the string of happily swims cat is happily\n",
      "the adverb in the string of cat happily swims is happily\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(reader)\n",
    "sentences, embeddings = reader.load_data('data/task1_dev_data.txt', vocabulary)\n",
    "print('\\n'.join([' '.join(sentence) for sentence in sentences]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa10041-ed34-4f6f-bf27-ccbb06172f15",
   "metadata": {},
   "source": [
    "As you can see, task 1 is the task of identifying the noun, verb, or adverb in a short three-word sentence.  The target POS (noun, verb, or adverb) is named somewhere in the input. The three-word sentence always contains the words \"cat swims happily,\" but in randomized order.\n",
    "\n",
    "Let's look at the embeddings for just the first of those sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "b4d60d4b-e69f-4d2f-bcf0-7a575a1b1b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f506d-6dc7-4e1e-8472-5a79faf8de03",
   "metadata": {},
   "source": [
    "As you can see, there is one word embedding per row (9 words in `sentences[0]`, 9 rows in `embeddings[0]`). Each word embedding is a one-hot vector, labeling the identity of the corresponding word.  For example, word 0 in the sentence is `the`, which is `vocabulary[8]`, so `embeddings[0,8]==1`. Likewise, word 1 in the sentence is `noun`, which is `vocabulary[3]`, so `embeddings[1,3]==1`.  And so on, down to the last word, `cat`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86c0e3-d9b8-41d9-a595-215e0fe2118f",
   "metadata": {},
   "source": [
    "### Task Definition\n",
    "\n",
    "In order to train and test a transformer, we need to know which words are the prompt text (from which we compute key and value vectors), which words are used to generate query vectors, and which words are the outputs (that the transformer tries to generate).\n",
    "\n",
    "In this MP, we will use a somewhat simple definition.  If a sentence contains $T$ words, we assume that:\n",
    "* The first $T-2$ words are the prompt text, from which keys and values are generated.\n",
    "* The last $2$ words are the target output that the transformer should generate.\n",
    "* In order to generate the second-to-last word of the sentence, the transformer uses a query based on the average of the embeddings of the prompt words.\n",
    "* In order to generate the last word, the transformer uses a query based on the average of the prompt words and the second-to-last word, i.e., the average of the first $T-1$ words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "68ed0af9-167a-4911-9b19-805a62e3e6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function define_task in module reader:\n",
      "\n",
      "define_task(embeddings)\n",
      "    Split the lexical embeddings into XK, XQ, and Y.\n",
      "    \n",
      "    @param:\n",
      "    embeddings - a T-by-V array, where T is length of the sentence, and V is size of vocabulary\n",
      "    \n",
      "    @return:\n",
      "    XK - a (T-2)-by-V array, with embeddings that are used to generate key and value vectors\n",
      "    XQ - a 2-by-V array, with embeddings that are used to generate query vectors\n",
      "    Y - a 2-by-V array, with embeddings that are the output targets for the transformer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(reader)\n",
    "help(reader.define_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "d8b92ff6-1697-4e0c-b512-fab3de2d5d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XK is:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "XQ is:\n",
      " 0.143 0.143 0.143 0.143 0.000 0.000 0.000 0.143 0.286 0.000 0.000 0.000\n",
      "0.125 0.125 0.125 0.125 0.000 0.000 0.125 0.125 0.250 0.000 0.000 0.000\n",
      "\n",
      "Y is:\n",
      " [[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "XK, XQ, Y = reader.define_task(embeddings[0])\n",
    "print('XK is:\\n',XK)\n",
    "print('\\nXQ is:\\n','\\n'.join([' '.join(['%.3f'%(x) for x in row]) for row in XQ]))\n",
    "print('\\nY is:\\n',Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6a2db4-898e-431b-8bc5-1b68a2e1d50b",
   "metadata": {},
   "source": [
    "As you can see, `XK` is just the first 7 words of the sentence, and `Y` is just the last two words.  \n",
    "\n",
    "`XQ` has two rows:\n",
    "* `XQ[0,:]==` average of the first 7 words of the sentence, including two occurrences of the word `the` (`vocabulary[8]`).\n",
    "* `XQ[1,:]==`average of the first 8 words of the sentence, including the 7 words of the prompt (`the noun in the happily swims cat`), plus the first word that the transformer is supposed to generate, the word `is` (`vocabulary[6]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb81ac-ff69-4f63-8d77-3b8f7dc92cf8",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "For the first task, you will not train a model.  Instead, you will test a model that has been designed by hand to be capable of solving this task.  A one-layer, single-headed transformer model consists of four matrices: $\\mathbf{W}_K$, $\\mathbf{W}_O$, $\\mathbf{W}_Q$, and $\\mathbf{W}_V$.  These are used to create the keys, outputs, queries, and value vectors, respectively, according to the equations:\n",
    "\n",
    "$$\\mathbf{K}=\\left[\\begin{array}{c}\\mathbf{k}_1^T\\\\\\vdots\\\\\\mathbf{k}_{T-2}^T\\end{array}\\right]=\\mathbf{X}_K\\mathbf{W}_K$$\n",
    "$$\\mathbf{O}=\\left[\\begin{array}{c}\\mathbf{o}_1^T\\\\\\mathbf{o}_2^T\\end{array}\\right]=\\mathbf{C}\\mathbf{W}_O$$\n",
    "$$\\mathbf{Q}=\\left[\\begin{array}{c}\\mathbf{q}_1^T\\\\\\mathbf{q}_2^T\\end{array}\\right]=\\mathbf{X}_Q\\mathbf{W}_Q$$\n",
    "$$\\mathbf{V}=\\left[\\begin{array}{c}\\mathbf{v}_1^T\\\\\\vdots\\\\\\mathbf{v}_{T-2}^T\\end{array}\\right]=\\mathbf{X}_K\\mathbf{W}_V$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "aba849ca-d0e5-4b9d-aba0-77cfb52e7fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_model in module reader:\n",
      "\n",
      "load_model(modelfile)\n",
      "    Load a model from a text file.\n",
      "    \n",
      "    @param:\n",
      "    modelfile (str) - name of the file to which model should be saved\n",
      "    @return:\n",
      "    WK - numpy array, size V-by-d where V is vocabulary size, d is embedding dimension\n",
      "    WO - numpy array, size d-by-V where V is vocabulary size, d is embedding dimension\n",
      "    WQ - numpy array, size V-by-d where V is vocabulary size, d is embedding dimension\n",
      "    WV - numpy array, size V-by-d where V is vocabulary size, d is embedding dimension\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(reader)\n",
    "help(reader.load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "e1888fe9-8aba-461e-9949-d072bb1bae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WK is\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "WO is\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "WV is\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(reader)\n",
    "WK, WO, WQ, WV = reader.load_model('data/task1_model.txt')\n",
    "print('WK is\\n',WK)\n",
    "print('WO is\\n',WO)\n",
    "print('WV is\\n',WV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc25b184-5601-4822-90cf-8a9575421e8e",
   "metadata": {},
   "source": [
    "As you can see, for this simple model, $\\mathbf{W}_K$ and $\\mathbf{W}_V$ are identity matrices, meaining that the key and value vectors are exactly equal to the input word embeddings.  \n",
    "\n",
    "$\\mathbf{W}_O$ is almost an identity matrix, except that it maps from the input word `in` (index 7) to the output word `is` (index 6).  Thus, if it finds that the context vector is the word `in`, then the output should be the word `is`.\n",
    "\n",
    "The matrix that is significantly non-trivial is $\\mathbf{W}_Q$, which is set up so that: \n",
    "* If the prompt contains `noun` but not `is`, then it generates a query vector that gives greatest weight to the word `in` (`WQ[3,7]==2.5` is the largest entry in its row).\n",
    "* If the prompt+generated first word contains the sequence `noun... is`, then it generates a query vector that gives greatest weight to the word `cat` (`WQ[3,0]+WQ[6,0]==3`, which is the largest entry in the sum of these two rows).\n",
    "* If the prompt contains `adverb` but not `is`, then it generates a query vector that gives greatest weight to the word `in` (`WQ[4,7]==2.5` is the largest entry in its row).\n",
    "* If the prompt+generated first word contains the sequence `adverb... is`, then it generates a query vector that gives greatest weight to the word `happily` (`WQ[4,1]+WQ[6,1]==3`, which is the largest entry in the sum of these two rows).\n",
    "* If the prompt contains `verb` but not `is`, then it generates a query vector that gives greatest weight to the word `in` (`WQ[5,6]==2.5` is the largest entry in its row).\n",
    "* If the prompt+generated first word contains the sequence `verb... is`, then it generates a query vector that gives greatest weight to the word `swims` (`WQ[5,2]+WQ[6,2]==3`, which is the largest entry in the sum of these two rows).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "9a659fc7-5ec7-4477-b320-439ac87a8d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WQ is\n",
      " [[-99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99. ]\n",
      " [-99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99. ]\n",
      " [-99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99. ]\n",
      " [  2.  -99.  -99.  -99.  -99.  -99.  -99.    2.5 -99.  -99.  -99.  -99. ]\n",
      " [-99.    2.  -99.  -99.  -99.  -99.  -99.    2.5 -99.  -99.  -99.  -99. ]\n",
      " [-99.  -99.    2.  -99.  -99.  -99.  -99.    2.5 -99.  -99.  -99.  -99. ]\n",
      " [  1.    1.    1.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99. ]\n",
      " [-99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99. ]\n",
      " [-99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99. ]\n",
      " [-99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99. ]\n",
      " [-99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99. ]\n",
      " [-99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99.  -99. ]]\n"
     ]
    }
   ],
   "source": [
    "print('WQ is\\n',WQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe77432-69f3-4587-8605-d9735a0d06e2",
   "metadata": {},
   "source": [
    "## Task 1: Test an existing transformer\n",
    "\n",
    "For task 1, you will test the existing transformer model that we just loaded.  In order to do that, you'll need to modify three functions in `transformer.py`:  `softmax,` `forward,` and `generate`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3343b36a-807e-46f8-8b46-0364e404f8a7",
   "metadata": {},
   "source": [
    "### softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7579ae5-d925-417e-9e47-3a154ee72ff6",
   "metadata": {},
   "source": [
    "The softmax of a row vector $\\mathbf{v}^T$ is a vector of the same dimensions as $\\mathbf{v}^T$ whose $j^{\\text{th}}$ element is:\n",
    "$$\\text{softmax}_j(\\mathbf{v})=\\frac{\\exp(v_j)}{\\sum_k \\exp(v_k)}$$\n",
    "For example, if the vector is $\\mathbf{v}=[0,1,0,0]^T$, then its softmax is\n",
    "$$\\text{softmax}([0,1,0,0]^T)=\\left[\\frac{1}{3+e},\\frac{e}{3+e},\\frac{1}{3+e},\\frac{1}{3+e}\\right]^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "3687d3d5-7d99-4aee-93d5-765245c616b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1748777  0.47536689 0.1748777  0.1748777 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import transformer\n",
    "v = np.array([0,1,0,0])\n",
    "print(np.exp(v)/sum(np.exp(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeed10c8-f8de-46c9-a30d-e37fb9ef1516",
   "metadata": {},
   "source": [
    "Write the function `transformer.softmax` so it computes the result shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "30d5349e-d18e-415a-be9b-1cc7ee67d8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1748777 , 0.47536689, 0.1748777 , 0.1748777 ])"
      ]
     },
     "execution_count": 589,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "transformer.softmax([0,1,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eaf621-7eac-4243-ae02-128c6863cbb0",
   "metadata": {},
   "source": [
    "That's all there is to it, except... Except that the `np.exp` function can quickly lead to numerical overflow if its input is even moderately large.  For example, consider the vector $\\mathbf{v}=[990,991,990,990]^T$.  Its softmax is\n",
    "$$\\text{softmax}([990,991,990,990]^T)=\\left[\n",
    "\\frac{e^{990}}{e^{990}+e^{991}+e^{990}+e^{990}},\n",
    "\\frac{e^{991}}{e^{990}+e^{991}+e^{990}+e^{990}},\\ldots\\right]^T$$\n",
    "$$=\\left[\n",
    "\\frac{e^{990}}{e^{990}}\\times\\left(\\frac{1}{3+e}\\right),\n",
    "\\frac{e^{990}}{e^{990}}\\times\\left(\\frac{e}{3+e}\\right),\\ldots\\right]^T$$\n",
    "$$=\\left[\\frac{1}{3+e},\\frac{e}{3+e},\\frac{1}{3+e},\\frac{1}{3+e}\\right]^T$$\n",
    "\n",
    "When we try to compute it, though, we get the following error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "a7620890-e420-46ad-a15c-7c5f1a911330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ezhan\\AppData\\Local\\Temp\\ipykernel_18724\\3156646497.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.exp(v)/np.sum(np.exp(v)))\n",
      "C:\\Users\\Ezhan\\AppData\\Local\\Temp\\ipykernel_18724\\3156646497.py:2: RuntimeWarning: invalid value encountered in divide\n",
      "  print(np.exp(v)/np.sum(np.exp(v)))\n"
     ]
    }
   ],
   "source": [
    "v = [990,991,990,990]\n",
    "print(np.exp(v)/np.sum(np.exp(v)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfe79a-068f-4749-8348-5c864bca8430",
   "metadata": {},
   "source": [
    "We have encountered the following problem: $e^{990}$ is too large to be computed.\n",
    "\n",
    "A good solution is to subtract the maximum element from $\\mathbf{v}$ before computing the softmax.  That way, the largest element in the sum is always $e^0=1$, which is easy to represent on any computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8082033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "5666b361-82cd-49b4-a923-22497f8947ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The numerator of the softmax is\n",
      " [0.36787944 1.         0.36787944 0.36787944]\n",
      "\n",
      "and the softmax output is\n",
      " [0.1748777  0.47536689 0.1748777  0.1748777 ]\n"
     ]
    }
   ],
   "source": [
    "numerator = np.exp(v - np.amax(v))\n",
    "print('The numerator of the softmax is\\n',numerator)\n",
    "prob = numerator / np.sum(numerator)\n",
    "print('\\nand the softmax output is\\n',prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af5a2a3-9492-4b89-981b-79688dd9c7d1",
   "metadata": {},
   "source": [
    "Revise your code in `transformer.softmax` so that it works well even with very large logit vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "abd62435-259b-40b0-a4ea-8623ff3185e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1748777  0.47536689 0.1748777  0.1748777 ]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "print(transformer.softmax([990,991,990,990]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d05ffc-ec7a-4a37-a2b5-3f9df846153e",
   "metadata": {},
   "source": [
    "Finally, in order to compute attention, we will find it very useful to compute the softmax normalization on every row of a matrix.  Revise your code in `transformer.softmax` so that it performs the normalization separately on every row of an input matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "93045640-8b11-4024-81f9-587ec518b3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1748777  0.47536689 0.1748777  0.1748777 ]\n",
      " [0.1748777  0.1748777  0.47536689 0.1748777 ]]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "print(transformer.softmax([[990,991,990,990],[0,0,1,0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd250ec-a358-4026-92b9-a67b12e59300",
   "metadata": {},
   "source": [
    "### forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cdbd68-4db8-4a15-85d6-c4cc6797f78e",
   "metadata": {},
   "source": [
    "Now we are ready to implement the forward pass of a transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "1d6f9213-2878-4470-9044-5342390a7fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function forward in module transformer:\n",
      "\n",
      "forward(XK, XQ, WK, WO, WQ, WV)\n",
      "    Perform one layer of transformer inference, using trained model, on given data.\n",
      "    \n",
      "    @param:\n",
      "    XK - (T-2)-by-V array containing embeddings of words to be used for keys and values\n",
      "    XQ - 2-by-V array containing embeddings of words to be used for queries\n",
      "    WK - V-by-d array mapping X to K\n",
      "    WO - d-by-V array mapping C to O\n",
      "    WQ - V-by-d array mapping X to Q\n",
      "    WV - V-by-d array mapping X to V\n",
      "    \n",
      "    @return:\n",
      "    A - 2-by-(T-2) array, A[i,j] is attention the i'th query pays to the j'th key\n",
      "    C - 2-by-d array, context vectors from which P is computed\n",
      "    K - (T-2)-by-d array, key vectors computed from XK\n",
      "    O - 2-by-V array, O[i,j] is probability that i'th output word should be j\n",
      "    Q - 2-by-d array, query vectors computed from XQ\n",
      "    V - (T-2)-by-d array, value vectors computed from XK\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(transformer.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89e199-f37e-41eb-b03d-679feb84d7dc",
   "metadata": {},
   "source": [
    "After computing the matrices $\\mathbf{K}$, $\\mathbf{Q}$, and $\\mathbf{V}$, you need to compute the attention matrix $\\mathbf{A}$, the context matrix $\\mathbf{C}$, the output logist $\\mathbf{O}$, and the output probabilities $\\mathbf{P}$ according to:\n",
    "\n",
    "$$\\mathbf{A}=\\text{softmax}(\\mathbf{Q}\\mathbf{K}^T)$$\n",
    "$$\\mathbf{C}=\\mathbf{A}\\mathbf{V}$$\n",
    "$$\\mathbf{O}=\\text{softmax}(\\mathbf{C}\\mathbf{W}_O)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a103c-31a5-494c-9847-42956c8b2702",
   "metadata": {},
   "source": [
    "If you try this using the task1 model and the first sentence of the task1 data, you should find that:\n",
    "* $\\mathbf{K}$ and $\\mathbf{V}$ are equal to the prompt embedding $\\mathbf{X}_K$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "275e19d9-4385-451a-b333-88d3f540cd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K is\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "V is\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "A, C, K, O, Q, V = transformer.forward(XK, XQ, WK, WO, WQ, WV)\n",
    "print('K is\\n',K)\n",
    "print('V is\\n',V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45815765-5272-422c-98d0-682a304ee5ff",
   "metadata": {},
   "source": [
    "You should also find that:\n",
    "\n",
    "* $\\mathbf{Q}$ is a two-row matrix whose maximum elements are the vocabulary indices of the word `in` in the first row (element 7), and the word `cat` in the second row (element 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "75aec68e-64f3-4a51-9dc2-57d609b5e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q is\n",
      " -84.57 -99.00 -99.00 -99.00 -99.00 -99.00 -99.00 -84.50 -99.00 -99.00 -99.00 -99.00\n",
      "-73.88 -86.50 -86.50 -99.00 -99.00 -99.00 -99.00 -86.31 -99.00 -99.00 -99.00 -99.00\n"
     ]
    }
   ],
   "source": [
    "print('Q is\\n','\\n'.join([' '.join(['%.2f'%(q) for q in row]) for row in Q]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a83f00-1810-4e46-a6c4-79ff768ba678",
   "metadata": {},
   "source": [
    "* $\\mathbf{A}$ is a two-row matrix whose maximum elements are pointers to the positions, in the input sentence, of the words `in` in the first row (element 2), and of the word `cat` in the second row (element 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "0a6f3ceb-efe2-4971-a41a-71f7f6f81f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A is\n",
      " 0.00 0.00 0.52 0.00 0.00 0.00 0.48\n",
      "0.00 0.00 0.00 0.00 0.00 0.00 1.00\n"
     ]
    }
   ],
   "source": [
    "print('A is\\n','\\n'.join([' '.join(['%.2f'%(q) for q in row]) for row in A]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7280df-d0e4-4d21-a08d-98c331f117a7",
   "metadata": {},
   "source": [
    "* $\\mathbf{C}$ is a two-row matrix whose maximum elements are the same as those of $\\mathbf{Q}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "9336b462-def7-4473-80a3-ae5bd4de027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C is\n",
      " 0.48 0.00 0.00 0.00 0.00 0.00 0.00 0.52 0.00 0.00 0.00 0.00\n",
      "1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00\n"
     ]
    }
   ],
   "source": [
    "print('C is\\n','\\n'.join([' '.join(['%.2f'%(q) for q in row]) for row in C]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d74d1-9234-4157-9a74-3a585e09aa29",
   "metadata": {},
   "source": [
    "* $\\mathbf{O}$ is a two-row matrix.  Each of its rows is a probability vector (non-negative, sums to one).  Maximum elements are the vocabulary indices of the word `is` in the first row (element 6) and the word `cat` in the second row (element 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "fa79e300-d4f0-4644-9841-1954686cffa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O is\n",
      " 0.12 0.08 0.08 0.08 0.08 0.08 0.13 0.08 0.08 0.08 0.08 0.08\n",
      "0.20 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07 0.07\n"
     ]
    }
   ],
   "source": [
    "print('O is\\n','\\n'.join([' '.join(['%.2f'%(q) for q in row]) for row in O]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679a965-1336-4ccb-8729-f52e31f442e7",
   "metadata": {},
   "source": [
    "## generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "cb591f0f-0558-4624-8a8b-5ab52a0e41b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function generate in module transformer:\n",
      "\n",
      "generate(embeddings, vocabulary, WK, WO, WQ, WV)\n",
      "    Perform inference on the provided embeddings, and report the generated sentences.\n",
      "    \n",
      "    @param:\n",
      "    embeddings - a list of one-hot embedding matrices, one per sentence\n",
      "    vocabulary - a list of words in the vocabulary\n",
      "    WK - V-by-d array mapping X to K\n",
      "    WO - d-by-V array mapping C to O\n",
      "    WQ - V-by-d array mapping X to Q\n",
      "    WV - V-by-d array mapping X to V\n",
      "    \n",
      "    @return:\n",
      "    generated - a list of generated sentences, each as a list of space-separated words.\n",
      "      The first T-2 words of each sentence should be vocabulary items indexed by the\n",
      "      argmax of the first T-2 embeddings.  The last 2 words of each sentence should be\n",
      "      vocabulary items indexed by the argmax of the two outputs computed by running\n",
      "      the transformer with the provided WK, WO, WQ, and WV.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "help(transformer.generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877fbe0b-e6d2-478d-8f24-4af000eef19f",
   "metadata": {},
   "source": [
    "The `generate` function should do the following things for each sentence:\n",
    "* Use `reader.define_task` to generate XK, XQ, and Y\n",
    "* Look up the vocabulary items corresponding to the argmax in each row of XK\n",
    "* Use `forward` to generate transformer outputs\n",
    "* Look up the vocabulary items corresponding to the argmax in each row of O\n",
    "\n",
    "If your code is working, it should be able to generate correct answers for all of the sentences in task1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "f9c8feb7-6ecd-4410-8482-e3e7cd7d3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: the noun in the happily swims cat is cat\n",
      "\tReference: the noun in the happily swims cat is cat  ---  0 errors\n",
      "Hypothesis: the adverb in the string of happily cat swims is happily\n",
      "\tReference: the adverb in the string of happily cat swims is happily  ---  0 errors\n",
      "Hypothesis: in the string happily swims cat the verb is swims\n",
      "\tReference: in the string happily swims cat the verb is swims  ---  0 errors\n",
      "Hypothesis: the noun in happily cat swims is cat\n",
      "\tReference: the noun in happily cat swims is cat  ---  0 errors\n",
      "Hypothesis: the noun in the string of swims happily cat is cat\n",
      "\tReference: the noun in the string of swims happily cat is cat  ---  0 errors\n",
      "Hypothesis: in the string of words swims cat happily the adverb is happily\n",
      "\tReference: in the string of words swims cat happily the adverb is happily  ---  0 errors\n",
      "Hypothesis: the verb in the string of words swims happily cat is swims\n",
      "\tReference: the verb in the string of words swims happily cat is swims  ---  0 errors\n",
      "Hypothesis: the adverb in the string of swims cat happily is happily\n",
      "\tReference: the adverb in the string of swims cat happily is happily  ---  0 errors\n",
      "Hypothesis: the noun in the string of words cat happily swims is cat\n",
      "\tReference: the noun in the string of words cat happily swims is cat  ---  0 errors\n",
      "Hypothesis: in the string of swims happily cat the noun is cat\n",
      "\tReference: in the string of swims happily cat the noun is cat  ---  0 errors\n",
      "Hypothesis: the noun in the string happily cat swims is cat\n",
      "\tReference: the noun in the string happily cat swims is cat  ---  0 errors\n",
      "Hypothesis: the adverb in the string of happily swims cat is happily\n",
      "\tReference: the adverb in the string of happily swims cat is happily  ---  0 errors\n",
      "Hypothesis: the adverb in the string swims cat happily is happily\n",
      "\tReference: the adverb in the string swims cat happily is happily  ---  0 errors\n",
      "Hypothesis: in swims cat happily the verb is swims\n",
      "\tReference: in swims cat happily the verb is swims  ---  0 errors\n",
      "Hypothesis: the verb in the string cat happily swims is swims\n",
      "\tReference: the verb in the string cat happily swims is swims  ---  0 errors\n",
      "Hypothesis: the adverb in the string of cat swims happily is happily\n",
      "\tReference: the adverb in the string of cat swims happily is happily  ---  0 errors\n",
      "Hypothesis: the verb in the string of happily cat swims is swims\n",
      "\tReference: the verb in the string of happily cat swims is swims  ---  0 errors\n",
      "Hypothesis: the noun in the string swims cat happily is cat\n",
      "\tReference: the noun in the string swims cat happily is cat  ---  0 errors\n",
      "Hypothesis: the adverb in the string of happily swims cat is happily\n",
      "\tReference: the adverb in the string of happily swims cat is happily  ---  0 errors\n",
      "Hypothesis: the adverb in the string of cat happily swims is happily\n",
      "\tReference: the adverb in the string of cat happily swims is happily  ---  0 errors\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "generated = transformer.generate(embeddings,vocabulary,WK,WO,WQ,WV)\n",
    "for ref,hyp in zip(sentences,generated):\n",
    "    print('Hypothesis:',' '.join(hyp))\n",
    "    print('\\tReference:',' '.join(ref),' --- ',sum([x!=y for x,y in zip(ref,hyp)]),'errors')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84532e52-04cc-426b-ac0f-f0191cb7a556",
   "metadata": {},
   "source": [
    "Just to make sure that your code is actually calculating outputs, try running it with completely random weight matrices.  You should get 2 errors in almost every sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "587d96dd-f73e-4251-b42a-0dcdd867643e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: the noun in the happily swims cat string string\n",
      "\tReference: the noun in the happily swims cat is cat  ---  2 errors\n",
      "Hypothesis: the adverb in the string of happily cat swims string string\n",
      "\tReference: the adverb in the string of happily cat swims is happily  ---  2 errors\n",
      "Hypothesis: in the string happily swims cat the verb string string\n",
      "\tReference: in the string happily swims cat the verb is swims  ---  2 errors\n",
      "Hypothesis: the noun in happily cat swims string string\n",
      "\tReference: the noun in happily cat swims is cat  ---  2 errors\n",
      "Hypothesis: the noun in the string of swims happily cat string string\n",
      "\tReference: the noun in the string of swims happily cat is cat  ---  2 errors\n",
      "Hypothesis: in the string of words swims cat happily the adverb string string\n",
      "\tReference: in the string of words swims cat happily the adverb is happily  ---  2 errors\n",
      "Hypothesis: the verb in the string of words swims happily cat string string\n",
      "\tReference: the verb in the string of words swims happily cat is swims  ---  2 errors\n",
      "Hypothesis: the adverb in the string of swims cat happily string string\n",
      "\tReference: the adverb in the string of swims cat happily is happily  ---  2 errors\n",
      "Hypothesis: the noun in the string of words cat happily swims string string\n",
      "\tReference: the noun in the string of words cat happily swims is cat  ---  2 errors\n",
      "Hypothesis: in the string of swims happily cat the noun string string\n",
      "\tReference: in the string of swims happily cat the noun is cat  ---  2 errors\n",
      "Hypothesis: the noun in the string happily cat swims string string\n",
      "\tReference: the noun in the string happily cat swims is cat  ---  2 errors\n",
      "Hypothesis: the adverb in the string of happily swims cat string string\n",
      "\tReference: the adverb in the string of happily swims cat is happily  ---  2 errors\n",
      "Hypothesis: the adverb in the string swims cat happily string string\n",
      "\tReference: the adverb in the string swims cat happily is happily  ---  2 errors\n",
      "Hypothesis: in swims cat happily the verb happily happily\n",
      "\tReference: in swims cat happily the verb is swims  ---  2 errors\n",
      "Hypothesis: the verb in the string cat happily swims string string\n",
      "\tReference: the verb in the string cat happily swims is swims  ---  2 errors\n",
      "Hypothesis: the adverb in the string of cat swims happily string string\n",
      "\tReference: the adverb in the string of cat swims happily is happily  ---  2 errors\n",
      "Hypothesis: the verb in the string of happily cat swims string string\n",
      "\tReference: the verb in the string of happily cat swims is swims  ---  2 errors\n",
      "Hypothesis: the noun in the string swims cat happily string string\n",
      "\tReference: the noun in the string swims cat happily is cat  ---  2 errors\n",
      "Hypothesis: the adverb in the string of happily swims cat string string\n",
      "\tReference: the adverb in the string of happily swims cat is happily  ---  2 errors\n",
      "Hypothesis: the adverb in the string of cat happily swims string string\n",
      "\tReference: the adverb in the string of cat happily swims is happily  ---  2 errors\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "V = len(vocabulary)\n",
    "generated = transformer.generate(\n",
    "    embeddings,\n",
    "    vocabulary,\n",
    "    np.random.uniform(0,1,(V,V)),\n",
    "    np.random.uniform(0,1,(V,V)),\n",
    "    np.random.uniform(0,1,(V,V)),\n",
    "    np.random.uniform(0,1,(V,V))\n",
    ")\n",
    "for ref,hyp in zip(sentences,generated):\n",
    "    print('Hypothesis:',' '.join(hyp))\n",
    "    print('\\tReference:',' '.join(ref),' --- ',sum([x!=y for x,y in zip(ref,hyp)]),'errors')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb303a9d-c1bd-4a1b-8a35-b3938b5175ed",
   "metadata": {},
   "source": [
    "## Task 2: Train a Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfcd2f6-9bfc-4104-8e6c-96e880bc059c",
   "metadata": {},
   "source": [
    "For task 2, you are not provided with a transformer.  However, you do have 100 sentences of training data!  The data are similar to those of task 1, but with a greater variety of nouns, verbs, and adverbs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "f53d997a-3cca-4689-98dd-d717c9fe14be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the noun in the string of happily cat programs is cat\n",
      "the adverb in the string happily dog swims is happily\n",
      "the verb in happily cat programs is programs\n",
      "in the string of words programs capybara strongly the adverb is strongly\n",
      "the noun in quickly programs dog is dog\n",
      "in the capybara paints strongly the verb is paints\n",
      "in the string strongly capybara programs the verb is programs\n",
      "in the strongly swims cat the noun is cat\n",
      "the noun in the string of words quickly capybara swims is capybara\n",
      "the verb in the swims quickly capybara is swims\n",
      "the verb in happily swims cat is swims\n",
      "the noun in the string of words strongly cat programs is cat\n",
      "the adverb in swims dog quickly is quickly\n",
      "the noun in the string of words programs happily dog is dog\n",
      "the adverb in the string cat swims strongly is strongly\n",
      "in capybara strongly programs the adverb is strongly\n",
      "in the string of quickly capybara swims the adverb is quickly\n",
      "the adverb in the dog paints happily is happily\n",
      "the noun in programs capybara quickly is capybara\n",
      "the verb in dog strongly programs is programs\n",
      "the noun in the swims quickly cat is cat\n",
      "the verb in the happily swims capybara is swims\n",
      "in the programs quickly capybara the adverb is quickly\n",
      "in the string happily programs capybara the adverb is happily\n",
      "the adverb in the string dog paints happily is happily\n",
      "in the string of dog happily programs the verb is programs\n",
      "the noun in the string of words cat happily swims is cat\n",
      "the adverb in the string of swims dog happily is happily\n",
      "the adverb in the string of words cat swims quickly is quickly\n",
      "in the string quickly paints cat the noun is cat\n",
      "in the string of words programs happily cat the adverb is happily\n",
      "the verb in the string of programs happily capybara is programs\n",
      "the noun in dog strongly programs is dog\n",
      "in the string quickly cat swims the verb is swims\n",
      "the verb in the string of strongly dog programs is programs\n",
      "the verb in the string paints quickly capybara is paints\n",
      "the adverb in the string of words programs cat strongly is strongly\n",
      "the noun in happily swims cat is cat\n",
      "in the string of words quickly cat programs the verb is programs\n",
      "the adverb in swims cat strongly is strongly\n",
      "in the string of words swims capybara quickly the verb is swims\n",
      "the verb in the string cat paints happily is paints\n",
      "in the string of quickly swims capybara the noun is capybara\n",
      "the verb in the string happily programs dog is programs\n",
      "in the string of capybara strongly paints the noun is capybara\n",
      "the adverb in the string of words swims cat happily is happily\n",
      "the adverb in the string of capybara happily swims is happily\n",
      "in the string quickly swims cat the verb is swims\n",
      "the adverb in the dog programs happily is happily\n",
      "the noun in the string of words swims capybara happily is capybara\n",
      "in the string of words cat programs happily the noun is cat\n",
      "the adverb in the string happily dog swims is happily\n",
      "in the string capybara strongly swims the noun is capybara\n",
      "the adverb in the programs capybara quickly is quickly\n",
      "the verb in the string strongly paints capybara is paints\n",
      "in the string of words swims cat happily the noun is cat\n",
      "the adverb in the string of strongly swims capybara is strongly\n",
      "the verb in the cat programs quickly is programs\n",
      "the verb in the string of swims quickly capybara is swims\n",
      "the adverb in the swims strongly capybara is strongly\n",
      "the verb in the string of words capybara programs happily is programs\n",
      "the verb in the string of swims strongly capybara is swims\n",
      "the adverb in the string of words dog strongly programs is strongly\n",
      "the verb in paints quickly capybara is paints\n",
      "the adverb in dog programs strongly is strongly\n",
      "in cat paints happily the adverb is happily\n",
      "the verb in the string cat paints happily is paints\n",
      "in the string of words paints dog quickly the verb is paints\n",
      "the noun in the string of words cat programs quickly is cat\n",
      "the noun in the string dog quickly swims is dog\n",
      "the noun in the string dog happily swims is dog\n",
      "the adverb in the paints happily dog is happily\n",
      "the verb in the programs happily dog is programs\n",
      "in the string of words dog programs strongly the verb is programs\n",
      "the noun in the string swims strongly dog is dog\n",
      "in the string of words paints capybara quickly the verb is paints\n",
      "the adverb in the string of programs capybara strongly is strongly\n",
      "in programs capybara quickly the adverb is quickly\n",
      "the noun in dog strongly programs is dog\n",
      "in the paints capybara happily the noun is capybara\n",
      "the adverb in the string of programs happily cat is happily\n",
      "the noun in the string of words dog programs quickly is dog\n",
      "the noun in the string of cat happily programs is cat\n",
      "in the string of words paints cat quickly the verb is paints\n",
      "in the string cat programs happily the adverb is happily\n",
      "the adverb in paints happily dog is happily\n",
      "the verb in the happily swims dog is swims\n",
      "the noun in the string happily swims cat is cat\n",
      "the noun in the string programs cat happily is cat\n",
      "in the quickly swims cat the noun is cat\n",
      "the verb in quickly dog paints is paints\n",
      "in the string of cat paints strongly the noun is cat\n",
      "in dog quickly paints the adverb is quickly\n",
      "in the string of words strongly capybara paints the verb is paints\n",
      "the noun in the strongly capybara programs is capybara\n",
      "the adverb in happily capybara swims is happily\n",
      "in the string of words cat paints quickly the verb is paints\n",
      "in the dog paints strongly the noun is dog\n",
      "the adverb in the string of paints strongly cat is strongly\n",
      "in the strongly programs cat the verb is programs\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(reader)\n",
    "vocabulary = reader.load_vocabulary('data/task2_vocabulary.txt')\n",
    "sentences, embeddings = reader.load_data('data/task2_train_data.txt', vocabulary)\n",
    "print('\\n'.join([' '.join(s) for s in sentences]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55447028-b68d-42d8-bd29-b3b564175015",
   "metadata": {},
   "source": [
    "In order to perform training, you'll need to write three methods: `cross_entropy_loss`, `gradient`, and `train`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bf14f9-5397-4bc9-8909-2886518712d6",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e979905-97b9-4bc8-9f00-4ce6cdc0b82e",
   "metadata": {},
   "source": [
    "First, write a function called `loss` that calculates the loss and the derivative of the loss.  Given a transformer output $\\mathbf{O}$ whose elements are $o_{i,j}\\in[0,1]$ and a corresponding set of one-hot target vectors $\\mathbf{Y}$ whose elements are $y_{i,j}\\in\\{0,1\\}$, the cross-entropy loss is\n",
    "\n",
    "$$\\mathcal{L}=-\\sum_i y_{i,j}\\ln \\max\\left(o_{i,j},\\epsilon\\right)$$\n",
    "\n",
    "where $\\epsilon$ is the python constant `sys.float_info.min`, and is used here just to make sure we don't get floating-point underflow. \n",
    "\n",
    "The derivatives of cross-entropy with respect to $o_{i,j}$ and $\\mathbf{O}$ are\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial o_{i,j}}=-\\frac{y_{i,j}}{o_{i,j}},~~~\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{O}}=-\\frac{\\mathbf{Y}}{\\mathbf{O}},\n",
    "$$\n",
    "where the division is an element-wise or \"array\" division (https://numpy.org/doc/2.2/reference/generated/numpy.divide.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "c0eef412-ff46-4559-a0c0-a3e7fbb634b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cross_entropy_loss in module transformer:\n",
      "\n",
      "cross_entropy_loss(O, Y)\n",
      "    Calculate losses from network outputs O if target one-hot vectors are Y.\n",
      "    \n",
      "    @param:\n",
      "    O - NQ-by-V array.  O[n,v]=probability that n'th output is v.\n",
      "    Y - NQ-by-V array. Y[n,v]=1 if n'th target is v, else Y[n,v]=0.\n",
      "    \n",
      "    @return:\n",
      "    L - cross-entropy loss, summed over all rows\n",
      "    dO - NQ-by-V array.  Derivatives of the loss with respect to the elements of O.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "help(transformer.cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd550ad0-2f27-4d24-bf07-b28fc1fbcbe4",
   "metadata": {},
   "source": [
    "For example, suppose we look up $\\mathbf{Y}$ for the first train embedding, randomly generate a (correctly-normalized) probability matrix of the same size, then find the resulting loss and loss gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "fa322b09-174c-417c-af65-f53acfefbd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y is\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "O is\n",
      "0.09 0.0061 0.098 0.082 0.094 0.017 0.004 0.039 0.098 0.038 0.023 0.064 0.091 0.094 0.058 0.0062 0.068 0.029\n",
      "0.0018 0.074 0.095 0.098 0.1 0.054 0.1 0.03 0.028 0.088 0.053 0.0019 0.062 0.074 0.041 0.015 0.059 0.023\n"
     ]
    }
   ],
   "source": [
    "XK, XQ, Y = reader.define_task(embeddings[0])\n",
    "print('Y is\\n',Y)\n",
    "\n",
    "O = np.random.uniform(0, 100, size=Y.shape)\n",
    "O /= np.expand_dims(np.sum(O,axis=1), axis=1)\n",
    "print('O is')\n",
    "print('\\n'.join([' '.join(['%.2g'%(o) for o in row]) for row in O]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "9db29707-9418-41c4-9397-b7876df39615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss is 8.7240619906924 which should equal 8.7240619906924\n",
      "Its derivative dL/dO is\n",
      "-0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -11 -0 -0 -0 -0 -0\n",
      "-5.6e+02 -0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -0 -0\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "L, dO = transformer.cross_entropy_loss(O,Y)\n",
    "print('Total loss is',L, 'which should equal', -np.log(O[0,12])-np.log(O[1,0]))\n",
    "print('Its derivative dL/dO is')\n",
    "print('\\n'.join([' '.join(['%.2g'%(o) for o in row]) for row in dO]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64ead2-51da-4aab-afc4-6a4ec6aaa100",
   "metadata": {},
   "source": [
    "### Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b6c2c-649f-410a-8f89-f7af8b063966",
   "metadata": {},
   "source": [
    "Now that we know the loss (and its derivative with respect to $O$), we need to find the derivative of the loss with respect to all of the model parameters: $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_K}$, $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_O}$, $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_Q}$, and $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_V}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a99bd-b81f-4706-a99b-e5ea7bf9f885",
   "metadata": {},
   "source": [
    "In order to calculate those derivatives, you will want to use the chain rule.  In order to do that, you might find it useful to think about the two types of layer that are used repeatedly in the transformer: matrix multiplication, and the softmax function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e31f014-3aea-4b82-9588-3a1e3baba8c0",
   "metadata": {},
   "source": [
    "#### Matrix Multiplication\n",
    "\n",
    "Suppose you have three matrices, $\\mathbf{A}$, $\\mathbf{B}$, and $\\mathbf{C}$, whose elements are $a_{i,j}$, $b_{j,k}$, and $c_{i,k}$, and suppose that\n",
    "\n",
    "$$\\mathbf{C}=\\mathbf{A}\\mathbf{B} ~~~\\Leftrightarrow~~~ c_{i,k}=\\sum_j a_{i,j}b_{j,k}$$\n",
    "\n",
    "Suppose you know $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{C}}$, whose elements are $\\frac{\\partial\\mathcal{L}}{\\partial c_{i,k}}$.  Then\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial a_{i,j}}=\\sum_k\\frac{\\partial\\mathcal{L}}{\\partial c_{i,k}}b_{j,k}~~~\\Leftrightarrow~~~ \\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{A}}=\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{C}}\\mathbf{B}^T$$\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial b_{j,k}}=\\sum_i a_{i,j}\\frac{\\partial\\mathcal{L}}{\\partial c_{i,k}}~~~\\Leftrightarrow~~~ \\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{B}}=\\mathbf{A}^T\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{C}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c58645-3780-4a5c-941b-27b17c3109ce",
   "metadata": {},
   "source": [
    "#### Softmax\n",
    "\n",
    "Suppose you have two matrices, $\\mathbf{A}$ and $\\mathbf{Z}$, whose elements are $a_{i,j}$ and $z_{i,j}$, and suppose that\n",
    "\n",
    "$$\\mathbf{A}=\\text{softmax}\\left(\\mathbf{Z}\\right) ~~~\\Leftrightarrow~~~ a_{i,j}=\\frac{e^{z_{i,j}}}{\\sum_\\ell e^{z_{i,\\ell}}}$$\n",
    "\n",
    "Suppose you know $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{A}}$, whose elements are $\\frac{\\partial\\mathcal{L}}{\\partial a_{i,j}}$.  Then\n",
    "\n",
    "$$\\frac{\\partial\\mathcal{L}}{\\partial z_{m,n}}=\\sum_k\\frac{\\partial\\mathcal{L}}{\\partial a_{m,k}}\\frac{\\partial a_{m,k}}{\\partial z_{m,n}}=a_{m,n}\\left(\\frac{\\partial\\mathcal{L}}{\\partial a_{m,n}}-\\sum_k a_{m,k}\\frac{\\partial\\mathcal{L}}{\\partial a_{m,k}}\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba08eef-be02-43b3-a2bd-3edfaab4bc6b",
   "metadata": {},
   "source": [
    "The idea of the `gradient` function is to apply the above formulas, one by one backward through the transformer, until you are able to calculate the desired derivatives $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_K}$, $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_O}$, $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_Q}$, and $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{W}_V}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "e4f086ff-1327-463f-b697-3eead5e2815e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function gradient in module transformer:\n",
      "\n",
      "gradient(XK, XQ, Y, WK, WO, WQ, WV, A, C, K, O, Q, V)\n",
      "    Compute gradient of cross-entropy loss with respect to WK, WO, WQ, and WV\n",
      "    given the input data in K, Q, and V, and the target outputs in Y.\n",
      "    \n",
      "    @param:\n",
      "    XK - one embedding per row, first n-2 words in the sentence\n",
      "    XQ - one embedding per row, 3rd-from-last and 2nd-from-last words in the sentence\n",
      "    Y - one embedding per row, last two words in the sentence\n",
      "    O - 2-by-V array, O[i,j] is probability that i'th output word should be j\n",
      "    C - 2-by-d array, context vectors from which O is computed\n",
      "    V - (T-2)-by-d array, value vectors of which each row of C is a weighted average\n",
      "    A - 2-by-(T-2) array, A[i,j] is attention the i'th query pays to the j'th key\n",
      "    K - (T-2)-by-d array, key vectors computed from XK\n",
      "    Q - 2-by-d array, query vectors computed from XQ\n",
      "    \n",
      "    @return:\n",
      "    dWK - gradient of cross-entropy with respect to WK\n",
      "    dWO - gradient of cross-entropy with respect to WO\n",
      "    dWQ - gradient of cross-entropy with respect to WQ\n",
      "    dWV - gradient of cross-entropy with respect to WV\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "help(transformer.gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c58fd-3847-4464-a959-b26fb0e46445",
   "metadata": {},
   "source": [
    "Without writing the `train` function, the only way to test your `gradient` function is by calculating the desired values (e.g., by hand, or using some other code), and then comparing them to the values your code produces.  In order to help you out, we've provided some examples of desired values in the function `test/test_visible.py,` which you can access using python's unittest facility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "ac4c704e-1abe-40e5-84ee-a1767ef88357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E\n",
      "======================================================================\n",
      "ERROR: test_visible (unittest.loader._FailedTest)\n",
      "----------------------------------------------------------------------\n",
      "ImportError: Failed to import test module: test_visible\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\unittest\\loader.py\", line 436, in _find_test_path\n",
      "    module = self._get_module_from_name(name)\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\unittest\\loader.py\", line 377, in _get_module_from_name\n",
      "    __import__(name)\n",
      "  File \"c:\\Users\\Ezhan\\OneDrive\\Desktop\\cs440\\MP4\\tests\\test_visible.py\", line 2, in <module>\n",
      "    from gradescope_utils.autograder_utils.decorators import weight\n",
      "ModuleNotFoundError: No module named 'gradescope_utils'\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 0.002s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=1 failures=0>"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "unittest.TextTestRunner().run(unittest.defaultTestLoader.discover('tests'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e4c626-fcfc-4d89-b394-cba500e4f232",
   "metadata": {},
   "source": [
    "### Train the transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beeb514-c2c8-4459-962f-3a8f8c0e4bdc",
   "metadata": {},
   "source": [
    "The `train` function should use stochastic gradient descent to train the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "d9e07e66-f013-443a-bba6-258842938fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module transformer:\n",
      "\n",
      "train(embeddings, WK, WO, WQ, WV, learningrate, num_iters)\n",
      "    Train a transformer using stochastic gradient descent (SGD).\n",
      "    Each iteration of SGD should choose one training sentence, uniformly at random,\n",
      "    compute the loss and loss gradient for that one sentence,\n",
      "    then adjust the parameters WK, WO, WQ and WV in the direction of the negative\n",
      "    gradient scaled by the learningrate.\n",
      "    \n",
      "    @param:\n",
      "    embeddings - embeddings[i][j,:] is one-hot vector of the j'th word in the i'th training sentence\n",
      "    WK - the matrix that multiplies each embedding to produce a key\n",
      "    WO - the matrix that multiplies the context vector to produce an output logit vector\n",
      "    WQ - the matrix that multiplies each embedding to produce a query\n",
      "    WV - the matrix that multiplies each embedding to produce a value\n",
      "    learningrate - scalar learning rate\n",
      "    num_iters - number of iterations of SGD to perform\n",
      "    \n",
      "    @return:\n",
      "    losses - losses[t]=cross-entropy loss of t'th iteration\n",
      "    WK - what WK has become after num_iters of training\n",
      "    WO - what WO has become after num_iters of training\n",
      "    WQ - what WQ has become after num_iters of training\n",
      "    WV - what WV has become after num_iters of training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(transformer)\n",
    "help(transformer.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3295f605-6a53-440e-8bc8-9f31236d4642",
   "metadata": {},
   "source": [
    "The parameters can be initialized randomly, e.g., using np.random.normal with a small standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "c8da7af1-f138-4f3f-9d4c-f5ce066e9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = len(vocabulary)\n",
    "WK0 = np.random.normal(loc=0,scale=0.001,size=(V,V))\n",
    "WO0 = np.random.normal(loc=0,scale=0.001,size=(V,V))\n",
    "WQ0 = np.random.normal(loc=0,scale=0.001,size=(V,V))\n",
    "WV0 = np.random.normal(loc=0,scale=0.001,size=(V,V))\n",
    "\n",
    "importlib.reload(transformer)\n",
    "losses, WK, WO, WQ, WV = transformer.train(embeddings,WK0,WO0,WQ0,WV0,0.01,50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "ecc3b647-54ad-4514-b00f-a1f49fe4cfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training losses, transformer MP task 2')"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(14,4))\n",
    "ax = fig.subplots(1)\n",
    "ax.plot(losses)\n",
    "ax.set_title('Training losses, transformer MP task 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31609497-acd1-49f0-9074-89e437011964",
   "metadata": {},
   "source": [
    "Training a transformer is kind of hard.  For this task, there tend to be three quite distinct training periods:\n",
    "* In the first training period, the transformer is assigning roughly equal probability to all words in the vocabulary, so the average loss is around $-2\\log(1/V)\\approx 6$.\n",
    "* In the second training period, the transformer has learned to produce the word `is` at the end of each sentence, but has not yet learned to produce the correct noun, verb, or adverb.  During this period the average loss is around $-\\log(1/V)\\approx 3$.\n",
    "* In the third training period, the transformer learns how to usually produce the correct noun, verb, or adverb.  At this point the average training loss drops to 0 on good iterations, with bad iterations that have much larger loss. Over time, the frequency of bad iterations continues to drop.\n",
    "\n",
    "If you don't see a training curve with all three periods, try adjusting your learning rate or num_iters until you get a curve with this behavior. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "799662f2-5804-4772-b058-ba1fb1118311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hypothesis: the noun in the happily swims cat is cat\n",
      "\tReference: the noun in the happily swims cat is cat  ---  0 errors\n",
      "Hypothesis: the verb in the string paints cat strongly is paints\n",
      "\tReference: the verb in the string paints cat strongly is paints  ---  0 errors\n",
      "Hypothesis: the verb in the string quickly programs capybara is swims\n",
      "\tReference: the verb in the string quickly programs capybara is programs  ---  1 errors\n",
      "Hypothesis: the adverb in the string of words capybara programs strongly is strongly\n",
      "\tReference: the adverb in the string of words capybara programs strongly is strongly  ---  0 errors\n",
      "Hypothesis: the adverb in the string swims quickly capybara is quickly\n",
      "\tReference: the adverb in the string swims quickly capybara is quickly  ---  0 errors\n",
      "Hypothesis: the adverb in the swims happily capybara is happily\n",
      "\tReference: the adverb in the swims happily capybara is happily  ---  0 errors\n",
      "Hypothesis: the noun in the string capybara strongly programs is capybara\n",
      "\tReference: the noun in the string capybara strongly programs is capybara  ---  0 errors\n",
      "Hypothesis: in the string of paints happily capybara the noun is capybara\n",
      "\tReference: in the string of paints happily capybara the noun is capybara  ---  0 errors\n",
      "Hypothesis: in the string strongly programs capybara the verb is programs\n",
      "\tReference: in the string strongly programs capybara the verb is programs  ---  0 errors\n",
      "Hypothesis: the verb in the capybara quickly programs is swims\n",
      "\tReference: the verb in the capybara quickly programs is programs  ---  1 errors\n",
      "Hypothesis: in the dog happily swims the noun is dog\n",
      "\tReference: in the dog happily swims the noun is dog  ---  0 errors\n",
      "Hypothesis: in the string cat happily swims the noun is cat\n",
      "\tReference: in the string cat happily swims the noun is cat  ---  0 errors\n",
      "Hypothesis: the noun in the swims dog quickly is dog\n",
      "\tReference: the noun in the swims dog quickly is dog  ---  0 errors\n",
      "Hypothesis: in the string happily cat swims the noun is cat\n",
      "\tReference: in the string happily cat swims the noun is cat  ---  0 errors\n",
      "Hypothesis: the adverb in paints cat strongly is strongly\n",
      "\tReference: the adverb in paints cat strongly is strongly  ---  0 errors\n",
      "Hypothesis: the noun in the string happily paints capybara is capybara\n",
      "\tReference: the noun in the string happily paints capybara is capybara  ---  0 errors\n",
      "Hypothesis: in the programs dog quickly the noun is dog\n",
      "\tReference: in the programs dog quickly the noun is dog  ---  0 errors\n",
      "Hypothesis: in the string paints happily cat the noun is cat\n",
      "\tReference: in the string paints happily cat the noun is cat  ---  0 errors\n",
      "Hypothesis: in the string of words capybara strongly swims the noun is capybara\n",
      "\tReference: in the string of words capybara strongly swims the noun is capybara  ---  0 errors\n",
      "Hypothesis: in the string of quickly swims dog the noun is dog\n",
      "\tReference: in the string of quickly swims dog the noun is dog  ---  0 errors\n"
     ]
    }
   ],
   "source": [
    "dev, devX = reader.load_data('data/task2_dev_data.txt', vocabulary)\n",
    "generated = transformer.generate(devX,vocabulary,WK,WO,WQ,WV)\n",
    "for ref,hyp in zip(dev,generated):\n",
    "    print('Hypothesis:',' '.join(hyp))\n",
    "    print('\\tReference:',' '.join(ref),' --- ',sum([x!=y for x,y in zip(ref,hyp)]),'errors')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c573b3-40e6-4f36-bc29-85d435f0ecdb",
   "metadata": {},
   "source": [
    "## Grading your work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80707835-114a-42cb-b2b3-ddd3c170febd",
   "metadata": {},
   "source": [
    "To grade your code, upload `transformer.py` to gradescope.  The autograder will run three types of tests:\n",
    "\n",
    "* `test_visible.py` is exactly the same as the code available to you in the template\n",
    "* `test_hidden.py` runs the same tests, but with different parameter values\n",
    "* `test_training.py` runs `transformer.train` for 50,000 iterations on a task that is similar to task 2, but with different vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e676184-90ed-4130-82bb-8fdf67ca183a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
